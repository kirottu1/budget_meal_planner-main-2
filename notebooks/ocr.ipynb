{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../outputs/extracted_images\n",
      "Directory already exists: ../outputs/extracted_images_ocr\n",
      "Directory already exists: ../outputs/extracted_products_ocr\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    \"../outputs/extracted_images\",\n",
    "    \"../outputs/extracted_images_ocr\",\n",
    "    \"../outputs/extracted_products_ocr\"\n",
    "]\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_image_with_overlap_simple(image, window_size=(500, 500), overlap=250):\n",
    "    height, width = image.shape[:2]\n",
    "    window_w, window_h = window_size\n",
    "    step_x = window_w - overlap\n",
    "    step_y = window_h - overlap\n",
    "   \n",
    "    # Calculate remainders to see how much padding is needed\n",
    "    remainder_x = width % step_x\n",
    "    remainder_y = height % step_y\n",
    "\n",
    "    child_images = []\n",
    "    # Loop through the image with the specified step size\n",
    "    for y in range(0, height - window_h + 1, step_y):\n",
    "        for x in range(0, width - window_w + 1, step_x):\n",
    "            # Extract the window and add it to the list\n",
    "            child_image = image[y:y + window_h, x:x + window_w]\n",
    "            child_images.append(child_image)  \n",
    "    return child_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_with_overlap_simple_padded_image(image, window_size=(500, 500), overlap=250):\n",
    "    height, width = image.shape[:2]\n",
    "    window_w, window_h = window_size\n",
    "    step_x = window_w - overlap\n",
    "    step_y = window_h - overlap\n",
    "\n",
    "    # Calculate remainders to see how much padding is needed\n",
    "    remainder_x = width % step_x\n",
    "    remainder_y = height % step_y\n",
    "\n",
    "    # Calculate padding amounts\n",
    "    pad_right = (step_x - remainder_x) if remainder_x > 0 else 0\n",
    "    pad_bottom = (step_y - remainder_y) if remainder_y > 0 else 0\n",
    "\n",
    "    # Apply padding\n",
    "    padded_image = cv2.copyMakeBorder(image, 0, pad_bottom, 0, pad_right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    padded_height, padded_width = padded_image.shape[:2]\n",
    "\n",
    "    child_images = []\n",
    "    # Loop through the padded image with the specified step size\n",
    "    for y in range(0, padded_height - window_h + 1, step_y):\n",
    "        for x in range(0, padded_width - window_w + 1, step_x):\n",
    "            # Extract the window and add it to the list\n",
    "            child_image = padded_image[y:y + window_h, x:x + window_w]\n",
    "            child_images.append(child_image)\n",
    "            \n",
    "    return child_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mostly_white_or_black(image, threshold=0.95, color='white'):\n",
    "    if color == 'white':\n",
    "        # Set target pixel value for white (255) and tolerance\n",
    "        target_value = 255\n",
    "        tolerance = 10  # Adjust as needed for brightness variations\n",
    "    elif color == 'black':\n",
    "        # Set target pixel value for black (0) and tolerance\n",
    "        target_value = 0\n",
    "        tolerance = 10  # Adjust as needed for darkness variations\n",
    "    else:\n",
    "        raise ValueError(\"Color should be 'white' or 'black'\")\n",
    "\n",
    "    # Convert to grayscale if it's a color image\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Create a binary mask for pixels within the target value Â± tolerance\n",
    "    if color == 'white':\n",
    "        mask = cv2.inRange(gray_image, target_value - tolerance, target_value)\n",
    "    else:\n",
    "        mask = cv2.inRange(gray_image, target_value, target_value + tolerance)\n",
    "\n",
    "    # Calculate the percentage of target pixels\n",
    "    target_pixel_ratio = np.sum(mask > 0) / mask.size\n",
    "\n",
    "    return target_pixel_ratio >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your big image\n",
    "for page_num in range(1, 26):\n",
    "    input_image= directories[0] + f\"/page_{page_num}.png\"\n",
    "    big_image = cv2.imread(input_image)  # Load your large image\n",
    "    child_images = split_image_with_overlap_simple(big_image, window_size=(400, 400), overlap=100)\n",
    "    child_iamge_path = directories[1] + f\"/page_{page_num}\"\n",
    "    if not os.path.exists(child_iamge_path):\n",
    "        os.makedirs(child_iamge_path)\n",
    "\n",
    "    for idx, child_image in enumerate(child_images):\n",
    "        is_white = is_mostly_white_or_black(child_image, threshold=0.95, color='white')\n",
    "        is_black = is_mostly_white_or_black(child_image, threshold=0.95, color='black')\n",
    "        if is_white or is_black:\n",
    "            continue\n",
    "        cv2.imwrite(f\"{child_iamge_path}/{idx}.png\", child_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_overlap(box1, box2):\n",
    "    \"\"\"Calculate the intersection-over-union (IoU) between two boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "\n",
    "    # Compute the intersection area\n",
    "    inter_x1 = max(x1, x3)\n",
    "    inter_y1 = max(y1, y3)\n",
    "    inter_x2 = min(x2, x4)\n",
    "    inter_y2 = min(y2, y4)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "    # Compute areas of the individual boxes\n",
    "    area_box1 = (x2 - x1) * (y2 - y1)\n",
    "    area_box2 = (x4 - x3) * (y4 - y3)\n",
    "\n",
    "    # Calculate overlap ratio of box2 within box1\n",
    "    overlap_ratio = inter_area / area_box2 if area_box2 > 0 else 0\n",
    "    return overlap_ratio\n",
    "\n",
    "def get_union_box(window_box, boxes, max_size=1000):\n",
    "    \"\"\"Get the union of the window box and all overlapping boxes with size limit during computation.\"\"\"\n",
    "    x1, y1, x2, y2 = window_box\n",
    "\n",
    "    for box in boxes:\n",
    "        bx1, by1, bx2, by2 = box\n",
    "\n",
    "        # Calculate potential new boundaries if we include this box\n",
    "        new_x1, new_y1 = min(x1, bx1), min(y1, by1)\n",
    "        new_x2, new_y2 = max(x2, bx2), max(y2, by2)\n",
    "\n",
    "        # Check if the new boundaries would exceed max_size\n",
    "        if (new_x2 - new_x1 <= max_size) and (new_y2 - new_y1 <= max_size):\n",
    "            # Update the union bounds only if within max_size constraint\n",
    "            x1, y1, x2, y2 = new_x1, new_y1, new_x2, new_y2\n",
    "\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def split_image_with_overlap(image, boxes, window_size=(500, 500), overlap=250, max_size=1000):\n",
    "    \"\"\"Split image with overlapping windows and return regions with text coverage.\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    window_w, window_h = window_size\n",
    "    step_x = window_w - overlap\n",
    "    step_y = window_h - overlap\n",
    "\n",
    "    # Calculate remainders to see how much padding is needed\n",
    "    remainder_x = width % step_x\n",
    "    remainder_y = height % step_y\n",
    "    \n",
    "    # Calculate padding amounts\n",
    "    pad_right = (step_x - remainder_x) if remainder_x > 0 else 0\n",
    "    pad_bottom = (step_y - remainder_y) if remainder_y > 0 else 0\n",
    "\n",
    "    # Apply padding\n",
    "    padded_image = cv2.copyMakeBorder(image, 0, pad_bottom, 0, pad_right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    height, width = padded_image.shape[:2]  # Update dimensions after padding\n",
    "\n",
    "    # Store the extracted regions\n",
    "    extracted_regions = []\n",
    "\n",
    "    # Loop through the image with the specified step size\n",
    "    for y in range(0, height - window_h + 1, step_y):\n",
    "        for x in range(0, width - window_w + 1, step_x):\n",
    "            # Define the current window box\n",
    "            window_box = (x, y, x + window_w, y + window_h)\n",
    "\n",
    "            # Check which boxes have >=60% overlap with this window\n",
    "            overlapping_boxes = [box for box in boxes if calculate_overlap(window_box, box) >= 0.6]\n",
    "\n",
    "            if overlapping_boxes:\n",
    "                # Calculate the union of the window and overlapping boxes\n",
    "                union_box = get_union_box(window_box, overlapping_boxes, max_size)\n",
    "\n",
    "                # Extract the region from the padded image using the union box\n",
    "                ux1, uy1, ux2, uy2 = union_box\n",
    "                extracted_region = padded_image[uy1:uy2, ux1:ux2]\n",
    "\n",
    "                # Add the extracted region and coordinates to the result list\n",
    "                extracted_regions.append({\n",
    "                    \"image\": extracted_region,\n",
    "                    \"coordinates\": union_box\n",
    "                })\n",
    "            else:\n",
    "                \n",
    "                ux1, uy1, ux2, uy2 = window_box\n",
    "                extracted_region = padded_image[uy1:uy2, ux1:ux2]\n",
    "                extracted_regions.append({\n",
    "                    \"image\": extracted_region,\n",
    "                    \"coordinates\": extracted_region\n",
    "                })\n",
    "\n",
    "\n",
    "    return extracted_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load the image for OCR processing.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def expand_box(box, padding, H, W):\n",
    "    \"\"\"Expand the bounding box by a padding amount.\"\"\"\n",
    "    (start_x, start_y, end_x, end_y) = box\n",
    "    start_x = max(0, start_x - padding)\n",
    "    start_y = max(0, start_y - padding)\n",
    "    end_x = min(W, end_x + padding)\n",
    "    end_y = min(H, end_y + padding)\n",
    "    return (start_x, start_y, end_x, end_y)\n",
    "\n",
    "def detect_and_extract_paragraphs(image_path, padding=10, paragraph=True, x_ths = 1.0, y_ths = 0.5, detail=1):\n",
    "    \"\"\"Detect and extract paragraphs using EasyOCR with paragraph mode.\"\"\"\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    image = load_image(image_path)\n",
    "    H, W = image.shape[:2]\n",
    "\n",
    "    # Run OCR detection with paragraph mode\n",
    "    ocr_results = reader.readtext(image, paragraph=paragraph, x_ths=x_ths, y_ths=y_ths, detail=detail)\n",
    "\n",
    "    # Extracted text results with expanded boxes\n",
    "    extracted_text = []\n",
    "    boxes = []  # List to store bounding boxes for clustering\n",
    "\n",
    "    for idx, result in enumerate(ocr_results):\n",
    "        # result[0] contains the bounding box (paragraph mode returns an enclosing box)\n",
    "        # result[1] contains the paragraph text\n",
    "        # result[2] is the confidence score\n",
    "                # Check if result contains 3 elements or just 2\n",
    "        if len(result) == 3:\n",
    "            bbox, text, prob = result\n",
    "        else:\n",
    "            bbox, text = result\n",
    "            prob = None  # Set confidence to None if not provided\n",
    "\n",
    "        # Convert bounding box to rectangular coordinates\n",
    "        start_x = int(min(point[0] for point in bbox))\n",
    "        start_y = int(min(point[1] for point in bbox))\n",
    "        end_x = int(max(point[0] for point in bbox))\n",
    "        end_y = int(max(point[1] for point in bbox))\n",
    "        \n",
    "        # Append the bounding box to the list\n",
    "        boxes.append((start_x, start_y, end_x, end_y))\n",
    "\n",
    "        # Append text results (without merging yet)\n",
    "        extracted_text.append({\n",
    "            \"text\": text,\n",
    "            \"box\": (start_x, start_y, end_x, end_y)\n",
    "        })\n",
    "\n",
    "    return extracted_text, boxes\n",
    "    \n",
    "\n",
    "def visualize_results(image_path, extracted_text, image_name):\n",
    "    \"\"\"Visualize extracted text regions on the image.\"\"\"\n",
    "    image = load_image(image_path)\n",
    "\n",
    "    for result in extracted_text:\n",
    "        (start_x, start_y, end_x, end_y) = result['box']\n",
    "        cv2.rectangle(image, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\n",
    "    cv2.imwrite(f'{directories[1]}/{image_name}', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your big image\n",
    "for page_num in range(1, 26):\n",
    "    process_image=f\"page_{page_num}\"\n",
    "    image_path = directories[0] + f\"/{process_image}.png\"\n",
    "    extracted_text, ocr_boxes = detect_and_extract_paragraphs(image_path, padding=0, paragraph=True, x_ths=.05, y_ths=0.3, detail=1) \n",
    "    visualize_results(image_path=image_path, extracted_text=extracted_text, image_name=f\"{process_image}_paragraph.png\")\n",
    "\n",
    "    input_image= directories[0] + f\"/page_{page_num}.png\"\n",
    "    big_image = cv2.imread(input_image)  # Load your large image\n",
    "    child_images = split_image_with_overlap(big_image, ocr_boxes, window_size=(400, 400), overlap=100, max_size=700)\n",
    "    child_iamge_path = directories[1] + f\"/page_{page_num}\"\n",
    "    if not os.path.exists(child_iamge_path):\n",
    "        os.makedirs(child_iamge_path)\n",
    "\n",
    "    for idx, extracted_image_info in enumerate(child_images):\n",
    "        child_image = extracted_image_info[\"image\"]\n",
    "        is_white = is_mostly_white_or_black(child_image, threshold=0.95, color='white')\n",
    "        is_black = is_mostly_white_or_black(child_image, threshold=0.95, color='black')\n",
    "        if is_white or is_black:\n",
    "            continue\n",
    "        cv2.imwrite(f\"{child_iamge_path}/{idx}.png\", child_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
